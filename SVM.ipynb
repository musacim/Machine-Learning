{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center><br>Implementing an SVM Classifier<br></center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this project, you are going to implement SVM. For this purpose, a data set (data.mat) is given to you. You can load the mat dataset into Python using the function `loadmat` in `Scipy.io`. When you load the data, you will obtain a dictionary object, where `X` stores the data matrix and `Y` stores the labels. You can use the first 150 samples for training and the rest for testing. In this project, you will use the software package [`LIBSVM`](http://www.csie.ntu.edu.tw/~cjlin/libsvm/) to implement SVM. Note that `LIBSVM` has a [`Python interface`](https://github.com/cjlin1/libsvm/tree/master/python), so you can call the SVM functions in Python. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - 30 pts\n",
    "\n",
    "Train a hard margin linear SVM and report both train and test classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "data=sio.loadmat(\"C:\\\\Users\\\\musat\\\\Desktop\\\\Yeni Klas√∂r\\\\data\")\n",
    "\n",
    "from libsvm import svmutil \n",
    "from svmutil import*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataY=data['Y']\n",
    "dataX=data['X']\n",
    "# splitting data into train and test part\n",
    "trainX=dataX[:150,:]\n",
    "trainY=dataY[:150,:]\n",
    "trainY=np.reshape(trainY,150)\n",
    "\n",
    "\n",
    "testX=dataX[150:,:]\n",
    "testY=dataY[150:,:]\n",
    "testY=np.reshape(testY,120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HardMargin=svm_train(trainY,trainX,'-c inf -t 0')  # we take C value as inf since, hard margin requires no Error tolerance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hard Margin SVM for training data \n",
      "\n",
      "Accuracy = 74.6667% (112/150) (classification)\n",
      "\n",
      "Hard Margin SVM for testing data \n",
      "\n",
      "Accuracy = 77.5% (93/120) (classification)\n"
     ]
    }
   ],
   "source": [
    "print(\"Hard Margin SVM for training data \\n\")      # Test and Train ACC results\n",
    "hardtrainresult=svm_predict(trainY,trainX,HardMargin) \n",
    "print(\"\\nHard Margin SVM for testing data \\n\")\n",
    "hardtestresult=svm_predict(testY,testX,HardMargin)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - 40 pts\n",
    "\n",
    "Train soft margin SVM for different values of the parameter $C$, and with different kernel functions. Systematically report your results. For instance, report the performances of different kernels for a fixed $C$, then report the performance for different $C$ values for a fixed kernel, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C value is fixed as 5, Kernel func is changeable.\n",
      "\n",
      "For Linear kernel function ACC is \n",
      "Accuracy = 81.6667% (98/120) (classification)\n",
      "For polynomial kernel function ACC is \n",
      "Accuracy = 80.8333% (97/120) (classification)\n",
      "For RBF kernel function ACC is \n",
      "Accuracy = 80% (96/120) (classification)\n",
      "For sigmoid kernel function ACC is \n",
      "Accuracy = 82.5% (99/120) (classification)\n",
      "\n",
      "Sigmoid is fixed as a kernel, C is changeable.\n",
      "\n",
      "For C=1 ACC is \n",
      "Accuracy = 82.5% (99/120) (classification)\n",
      "For C=3 ACC is \n",
      "Accuracy = 83.3333% (100/120) (classification)\n",
      "For C=5 ACC is \n",
      "Accuracy = 84.1667% (101/120) (classification)\n",
      "For C=7 ACC is \n",
      "Accuracy = 80.8333% (97/120) (classification)\n",
      "For C=9 ACC is \n",
      "Accuracy = 82.5% (99/120) (classification)\n"
     ]
    }
   ],
   "source": [
    "print(\"C value is fixed as 5, Kernel func is changeable.\\n\")\n",
    "SoftMargin=svm_train(trainY,trainX,'-c 5 -t 0')     # Choosing C is 5 and trying model for different kernels \n",
    "print(\"For Linear kernel function ACC is \")         # Linear, Polynomial, RBF, Sigmoid\n",
    "softtestresult=svm_predict(testY,testX,SoftMargin)\n",
    "\n",
    "SoftMargin=svm_train(trainY,trainX,'-c 5 -t 1')\n",
    "print(\"For polynomial kernel function ACC is \")\n",
    "softtestresult=svm_predict(testY,testX,SoftMargin)\n",
    "\n",
    "SoftMargin=svm_train(trainY,trainX,'-c 5 -t 2')\n",
    "print(\"For RBF kernel function ACC is \")\n",
    "softtestresult=svm_predict(testY,testX,SoftMargin)\n",
    "\n",
    "SoftMargin=svm_train(trainY,trainX,'-c 5 -t 3')\n",
    "print(\"For sigmoid kernel function ACC is \")\n",
    "softtestresult=svm_predict(testY,testX,SoftMargin)\n",
    "\n",
    "print(\"\\nSigmoid is fixed as a kernel, C is changeable.\\n\")\n",
    "                                                  #sigmoud function is fixed as a kernel func and we examine different C values \n",
    "SoftMargin=svm_train(trainY,trainX,'-c 3 -t 3')   # C values are 3, 6, 9, 12, 15.\n",
    "print(\"For C=1 ACC is \")\n",
    "softtestresult=svm_predict(testY,testX,SoftMargin)\n",
    "\n",
    "SoftMargin=svm_train(trainY,trainX,'-c 6 -t 3')\n",
    "print(\"For C=3 ACC is \")\n",
    "softtestresult=svm_predict(testY,testX,SoftMargin) \n",
    "\n",
    "SoftMargin=svm_train(trainY,trainX,'-c 9 -t 3')\n",
    "print(\"For C=5 ACC is \")\n",
    "softtestresult=svm_predict(testY,testX,SoftMargin) \n",
    "\n",
    "SoftMargin=svm_train(trainY,trainX,'-c 12 -t 3')\n",
    "print(\"For C=7 ACC is \")\n",
    "softtestresult=svm_predict(testY,testX,SoftMargin)\n",
    "\n",
    "SoftMargin=svm_train(trainY,trainX,'-c 15 -t 3')\n",
    "print(\"For C=9 ACC is \")\n",
    "softtestresult=svm_predict(testY,testX,SoftMargin)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 - 15 pts\n",
    "\n",
    "Please report how the number of support vectors changes as the value of $C$ increases (while all other parameters remain the same). Discuss whether your observations match the theory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Support Vectors for C=1 is  58\n",
      "# of Support Vectors for C=2 is  56\n",
      "# of Support Vectors for C=3 is  55\n",
      "# of Support Vectors for C=4 is  54\n"
     ]
    }
   ],
   "source": [
    "SoftMargin=svm_train(trainY,trainX,'-c 1 -t 0')\n",
    "SV=SoftMargin.get_SV()\n",
    "print(\"# of Support Vectors for C=1 is \",len(SV))\n",
    "\n",
    "SoftMargin=svm_train(trainY,trainX,'-c 2 -t 0')\n",
    "SV=SoftMargin.get_SV()\n",
    "print(\"# of Support Vectors for C=2 is \",len(SV))\n",
    "\n",
    "SoftMargin=svm_train(trainY,trainX,'-c 3 -t 0')\n",
    "SV=SoftMargin.get_SV()\n",
    "print(\"# of Support Vectors for C=3 is \",len(SV))\n",
    "\n",
    "SoftMargin=svm_train(trainY,trainX,'-c 4 -t 0')\n",
    "SV=SoftMargin.get_SV()\n",
    "print(\"# of Support Vectors for C=4 is \",len(SV))\n",
    "\n",
    "#number of sv is decreasing as we increase C value, it matches the theory\n",
    "#because, as C increases error tolerance decreases so hyperplanes complexity decreases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 - 15 pts\n",
    "\n",
    "Please investigate the changes in the hyperplane when you remove one of the support vectors, vs., one data point that is not a support vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear full model no exclution\n",
    "SoftMargin=svm_train(trainY,trainX,'-c 1 -t 0') \n",
    "\n",
    "#model that we exlude one support vector data point from dataset \n",
    "newtrainX=np.delete(trainX, 3, axis=0)\n",
    "newtrainY=np.delete(trainY, 3, axis=0)\n",
    "modelwithoutSV=svm_train(newtrainY,newtrainX,'-c 1 -t 0') \n",
    " \n",
    "#model that we exlude one non-support vector data point from dataset \n",
    "newtrainX2=np.delete(trainX, 6, axis=0)\n",
    "newtrainY2=np.delete(trainY, 6, axis=0)\n",
    "modelwithoutDP=svm_train(newtrainY2,newtrainX2,'-c 1 -t 0')\n",
    " \n",
    "def findWandB(model):  # we make the calculations here to find w and b, by using coefficients and support vectors via libsvm\n",
    "    b = model.rho\n",
    "    b=b[0]*-1\n",
    "    coef=model.get_sv_coef()\n",
    "    SV=model.get_SV()\n",
    "    SV_i=model.get_sv_indices()\n",
    "\n",
    "    Matrix=pd.DataFrame(SV)\n",
    "    Matrix=Matrix.fillna(0)\n",
    "    Vector=pd.DataFrame(coef)\n",
    "    colsize=len(Matrix.iloc[0,:])\n",
    "    rowsize=len(Matrix)\n",
    "\n",
    "    w=np.zeros([colsize-1,1])\n",
    "    for j in range(colsize-1):\n",
    "        sum=0\n",
    "        for i in range(rowsize):\n",
    "            sum+=Vector.iloc[i,0]*Matrix.iloc[i,j]\n",
    "        w[j]=sum\n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.47637751],\n",
       "        [ 0.39813231],\n",
       "        [ 0.86713369],\n",
       "        [ 0.57366213],\n",
       "        [ 0.32304567],\n",
       "        [ 0.10008082],\n",
       "        [ 0.05139059],\n",
       "        [-0.95512925],\n",
       "        [ 0.07086555],\n",
       "        [ 0.00576303],\n",
       "        [ 0.24894908],\n",
       "        [ 1.82218655],\n",
       "        [ 0.5129991 ]]),\n",
       " 1.1847368158140863)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findWandB(SoftMargin) # w and b values without exclution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.47637751],\n",
       "        [ 0.39813231],\n",
       "        [ 0.86713369],\n",
       "        [ 0.57366213],\n",
       "        [ 0.32304567],\n",
       "        [ 0.10008082],\n",
       "        [ 0.05139059],\n",
       "        [-0.95512925],\n",
       "        [ 0.07086555],\n",
       "        [ 0.00576303],\n",
       "        [ 0.24894908],\n",
       "        [ 1.82218655],\n",
       "        [ 0.5129991 ]]),\n",
       " 1.184736815814086)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findWandB(modelwithoutDP)  # w and b values without non-sv data point, as we can see excatly the same with full model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-2.94880396e-01],\n",
       "        [ 4.21184358e-01],\n",
       "        [ 9.26642823e-01],\n",
       "        [ 5.89842928e-01],\n",
       "        [ 3.48248290e-01],\n",
       "        [ 7.59411670e-02],\n",
       "        [-1.41871622e-03],\n",
       "        [-1.02161821e+00],\n",
       "        [ 1.01050049e-01],\n",
       "        [-1.44539506e-01],\n",
       "        [ 3.02006450e-01],\n",
       "        [ 1.81590860e+00],\n",
       "        [ 5.11221871e-01]]),\n",
       " 1.0716615962291343)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findWandB(modelwithoutSV) # w and b values without support vector data point, as we can see W and b is dramatically changed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus Task - 10 pts\n",
    "\n",
    "Use Python and [CVXOPT](http://cvxopt.org) QP solver to implement the hard margin SVM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxopt\n",
    "from cvxopt import matrix,solvers\n",
    "from cvxopt.solvers import qp\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  3.2653e-01  1.9592e+00  6e+00  2e+00  4e+00\n",
      " 1:  1.5796e+00  8.5663e-01  7e-01  0e+00  2e-15\n",
      " 2:  1.0195e+00  9.9227e-01  3e-02  1e-16  1e-15\n",
      " 3:  1.0002e+00  9.9992e-01  3e-04  2e-16  2e-15\n",
      " 4:  1.0000e+00  1.0000e+00  3e-06  3e-16  7e-16\n",
      " 5:  1.0000e+00  1.0000e+00  3e-08  4e-16  7e-16\n",
      "Optimal solution found.\n"
     ]
    }
   ],
   "source": [
    "Xtoy=[0,0,2,2,2,0,3,0] # we upload the example in class\n",
    "Xtoy=np.reshape(Xtoy,(4,2))\n",
    "Ytoy=[-1,-1,1,1]\n",
    "\n",
    "Q=np.zeros([3,3])  #creating Q,p,A,c matrices  \n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        if(i==j and i!=0):\n",
    "            Q[i][j]=1\n",
    "\n",
    "p=np.zeros([3,1])\n",
    "c=np.ones([4,1])\n",
    "\n",
    "A=np.zeros([4,3])\n",
    " \n",
    "for j in range(3):\n",
    "    for i in range(4):\n",
    "        if(j==0):\n",
    "            A[i][j]=Ytoy[i]\n",
    "    \n",
    "        else:\n",
    "            A[i][j]=Ytoy[i]*Xtoy[i][j-1]\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "U=solvers.qp(matrix(Q),matrix(p),matrix(-1*A),matrix(-1*c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0\n",
       "0 -1.0\n",
       "1  1.0\n",
       "2 -1.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(U['x'])  # we get results as we expect b*=-1 w*=[1,-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
